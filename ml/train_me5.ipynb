{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "OUTPUT_DIR = 'output_intfloat-multilingual-e5-large-extradata_baseline'\n",
    "if not os.path.exists(OUTPUT_DIR):\n",
    "    os.makedirs(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    apex = True\n",
    "    print_freq = 100\n",
    "    num_workers = 8\n",
    "    model = \"intfloat/multilingual-e5-large\"\n",
    "    gradient_checkpointing = True\n",
    "    scheduler = 'cosine'  # ['linear', 'cosine']\n",
    "    batch_scheduler = True\n",
    "    num_cycles = 0.5\n",
    "    num_warmup_steps = 0\n",
    "    epochs = 10\n",
    "    encoder_lr = 2e-5\n",
    "    decoder_lr = 2e-5\n",
    "    min_lr = 1e-6\n",
    "    eps = 1e-6\n",
    "    betas = (0.9, 0.999)\n",
    "    batch_size = 8\n",
    "    max_len = 512\n",
    "    weight_decay = 0.01\n",
    "    gradient_accumulation_steps = 1\n",
    "    max_grad_norm = 1000\n",
    "    target_cols = ['class']\n",
    "    seed = 42\n",
    "    n_fold = 5\n",
    "    trn_fold = [0, 1, 2, 3, 4]\n",
    "    train = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
    "from sklearn.metrics import f1_score\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim import AdamW\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm.auto import tqdm\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import re\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "# from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df):\n",
    "    df[\"text\"] = df[\"text\"].astype(str)\n",
    "    df[\"class\"] = df[\"class\"].astype(str)\n",
    "    df.drop_duplicates([\"class\", \"text\"], inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    df[\"text\"] = df[\"text\"].apply(lambda x: \" \".join(\n",
    "        re.findall(r\"[а-яА-Я0-9 ёЁ\\-\\.,?!+a-zA-Z]+\", x)))\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_score(y_trues, class_predictions):\n",
    "    class_predictions = [np.argmax(el) for el in class_predictions]\n",
    "\n",
    "    class_score = f1_score(y_trues[:, 0], class_predictions, average=\"macro\")\n",
    "    return class_score\n",
    "\n",
    "\n",
    "def get_logger(filename=os.path.join(OUTPUT_DIR, 'train')):\n",
    "    from logging import getLogger, INFO, StreamHandler, FileHandler, Formatter\n",
    "    logger = getLogger(__name__)\n",
    "    logger.setLevel(INFO)\n",
    "    handler1 = StreamHandler()\n",
    "    handler1.setFormatter(Formatter(\"%(message)s\"))\n",
    "    handler2 = FileHandler(filename=f\"{filename}.log\")\n",
    "    handler2.setFormatter(Formatter(\"%(message)s\"))\n",
    "    logger.addHandler(handler1)\n",
    "    logger.addHandler(handler2)\n",
    "    return logger\n",
    "\n",
    "\n",
    "LOGGER = get_logger()\n",
    "\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "seed_everything(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(501, 2)\n",
      "(673, 2)\n",
      "(193, 2)\n",
      "(1367, 2)\n",
      "(1221, 2)\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(\"dataset/sample.csv\")\n",
    "print(train.shape)\n",
    "extra_train = pd.read_csv(\"dataset/extra_sample.csv\")\n",
    "print(extra_train.shape)\n",
    "extra_train_v2 = pd.read_csv(\"dataset/extra_sample_v2.csv\")\n",
    "print(extra_train_v2.shape)\n",
    "train = pd.concat([train, extra_train, extra_train_v2], axis=0)\n",
    "print(train.shape)\n",
    "train = preprocess(train)\n",
    "print(train.shape)\n",
    "train[\"text\"] = train[\"text\"].apply(lambda x: \"query: \" + x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "class\n",
       "arrangement       324\n",
       "order             310\n",
       "contract          184\n",
       "application        72\n",
       "proxy              69\n",
       "act                69\n",
       "determination      50\n",
       "invoice            42\n",
       "bill               41\n",
       "statute            35\n",
       "contract offer     25\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[\"class\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_le = LabelEncoder()\n",
    "class_le.fit(train[\"class\"].tolist())\n",
    "train[\"class\"] = class_le.transform(train[\"class\"].tolist())\n",
    "\n",
    "with open(os.path.join(OUTPUT_DIR, \"executor_le.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(class_le, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>text</th>\n",
       "      <th>fold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>query: СОГЛАШЕНИЕ N 8 о расторжении трудового ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>query: Соглашение о предоставлении опциона на ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>query: Соглашение о реструктуризации задолженн...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>query: Дополнительное соглашение к договору ку...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>query: Соглашение о расторжении договора об ок...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   class                                               text  fold\n",
       "0      2  query: СОГЛАШЕНИЕ N 8 о расторжении трудового ...     2\n",
       "1      2  query: Соглашение о предоставлении опциона на ...     1\n",
       "2      2  query: Соглашение о реструктуризации задолженн...     2\n",
       "3      2  query: Дополнительное соглашение к договору ку...     0\n",
       "4      2  query: Соглашение о расторжении договора об ок...     2"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Fold = StratifiedKFold(n_splits=CFG.n_fold,\n",
    "                       shuffle=True, random_state=CFG.seed)\n",
    "for n, (train_index, val_index) in enumerate(Fold.split(train, train[\"class\"].tolist())):\n",
    "    train.loc[val_index, 'fold'] = int(n)\n",
    "train['fold'] = train['fold'].astype(int)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ea9c9a210f64f9796e3c5db158902b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1221 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1421 > 512). Running this sequence through the model will result in indexing errors\n",
      "max_len: 345518\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(CFG.model)\n",
    "tokenizer.save_pretrained(os.path.join(OUTPUT_DIR, 'tokenizer'))\n",
    "CFG.tokenizer = tokenizer\n",
    "\n",
    "lengths = []\n",
    "tk0 = tqdm(train['text'].fillna(\"\").values, total=len(train))\n",
    "for text in tk0:\n",
    "    length = len(tokenizer(text, add_special_tokens=False)['input_ids'])\n",
    "    lengths.append(length)\n",
    "CFG.max_len = max(lengths) + 2  # cls & sep\n",
    "LOGGER.info(f\"max_len: {CFG.max_len}\")\n",
    "CFG.max_len = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_input(cfg, text):\n",
    "    inputs = cfg.tokenizer.encode_plus(\n",
    "        text,\n",
    "        return_tensors=None,\n",
    "        add_special_tokens=True,\n",
    "        max_length=CFG.max_len,\n",
    "        pad_to_max_length=True,\n",
    "        truncation=True\n",
    "    )\n",
    "    for k, v in inputs.items():\n",
    "        inputs[k] = torch.tensor(v, dtype=torch.long)\n",
    "    return inputs\n",
    "\n",
    "\n",
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, cfg, df):\n",
    "        self.cfg = cfg\n",
    "        self.texts = df['text'].values\n",
    "        self.labels = df[\"class\"].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        inputs = prepare_input(self.cfg, self.texts[item])\n",
    "        label = torch.tensor(self.labels[item], dtype=torch.long)\n",
    "        return inputs, label\n",
    "\n",
    "\n",
    "def collate(inputs):\n",
    "    mask_len = int(inputs[\"attention_mask\"].sum(axis=1).max())\n",
    "    for k, v in inputs.items():\n",
    "        inputs[k] = inputs[k][:, :mask_len]\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_pool(last_hidden_states, attention_mask):\n",
    "    last_hidden = last_hidden_states.masked_fill(\n",
    "        ~attention_mask[..., None].bool(), 0.0)\n",
    "    return last_hidden.sum(dim=1) / attention_mask.sum(dim=1)[..., None]\n",
    "\n",
    "\n",
    "class CustomModel(nn.Module):\n",
    "    def __init__(self, cfg, config_path=None, pretrained=False):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        if config_path is None:\n",
    "            self.config = AutoConfig.from_pretrained(\n",
    "                cfg.model, output_hidden_states=True)\n",
    "            self.config.hidden_dropout = 0.\n",
    "            self.config.hidden_dropout_prob = 0.\n",
    "            self.config.attention_dropout = 0.\n",
    "            self.config.attention_probs_dropout_prob = 0.\n",
    "            LOGGER.info(self.config)\n",
    "        else:\n",
    "            self.config = torch.load(config_path)\n",
    "        if pretrained:\n",
    "            self.model = AutoModel.from_pretrained(\n",
    "                cfg.model, config=self.config)\n",
    "        else:\n",
    "            self.model = AutoModel(self.config)\n",
    "        if self.cfg.gradient_checkpointing:\n",
    "            self.model.gradient_checkpointing_enable()\n",
    "\n",
    "        self.fc = nn.Linear(self.config.hidden_size, 11)\n",
    "        self._init_weights(self.fc)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            module.weight.data.normal_(\n",
    "                mean=0.0, std=self.config.initializer_range)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            module.weight.data.normal_(\n",
    "                mean=0.0, std=self.config.initializer_range)\n",
    "            if module.padding_idx is not None:\n",
    "                module.weight.data[module.padding_idx].zero_()\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "\n",
    "    def feature_me5(self, inputs):\n",
    "        outputs = self.model(**inputs)\n",
    "        feature = average_pool(outputs.last_hidden_state,\n",
    "                               inputs['attention_mask'])\n",
    "        return feature\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        feature = self.feature_me5(inputs)\n",
    "        output = self.fc(feature)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (remain %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(fold, train_loader, model, criterion, optimizer, epoch, scheduler, device):\n",
    "    model.train()\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=CFG.apex)\n",
    "    losses = AverageMeter()\n",
    "    start = end = time.time()\n",
    "    global_step = 0\n",
    "    for step, (inputs, label) in enumerate(train_loader):\n",
    "        inputs = collate(inputs)\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = v.to(device)\n",
    "        label = label.to(device)\n",
    "\n",
    "        batch_size = label.size(0)\n",
    "        with torch.cuda.amp.autocast(enabled=CFG.apex):\n",
    "            pred = model(inputs)\n",
    "            loss = criterion(pred, label)\n",
    "        if CFG.gradient_accumulation_steps > 1:\n",
    "            loss = loss / CFG.gradient_accumulation_steps\n",
    "        losses.update(loss.item(), batch_size)\n",
    "        scaler.scale(loss).backward()\n",
    "        grad_norm = torch.nn.utils.clip_grad_norm_(\n",
    "            model.parameters(), CFG.max_grad_norm)\n",
    "        if (step + 1) % CFG.gradient_accumulation_steps == 0:\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "            global_step += 1\n",
    "            if CFG.batch_scheduler:\n",
    "                scheduler.step()\n",
    "        end = time.time()\n",
    "        if step % CFG.print_freq == 0 or step == (len(train_loader)-1):\n",
    "            print('Epoch: [{0}][{1}/{2}] '\n",
    "                  'Elapsed {remain:s} '\n",
    "                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n",
    "                  'Grad: {grad_norm:.4f}  '\n",
    "                  'LR: {lr:.8f}  '\n",
    "                  .format(epoch+1, step, len(train_loader),\n",
    "                          remain=timeSince(start, float(\n",
    "                              step+1)/len(train_loader)),\n",
    "                          loss=losses,\n",
    "                          grad_norm=grad_norm,\n",
    "                          lr=scheduler.get_lr()[0]))\n",
    "    return losses.avg\n",
    "\n",
    "\n",
    "def valid_fn(valid_loader, model, criterion, device):\n",
    "    losses = AverageMeter()\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    start = end = time.time()\n",
    "    for step, (inputs, label) in enumerate(valid_loader):\n",
    "        inputs = collate(inputs)\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = v.to(device)\n",
    "        label = label.to(device)\n",
    "\n",
    "        batch_size = label.size(0)\n",
    "        with torch.no_grad():\n",
    "            pred = model(inputs)\n",
    "            loss = criterion(pred, label)\n",
    "        if CFG.gradient_accumulation_steps > 1:\n",
    "            loss = loss / CFG.gradient_accumulation_steps\n",
    "        losses.update(loss.item(), batch_size)\n",
    "        preds.append(pred.to('cpu').numpy())\n",
    "        end = time.time()\n",
    "        if step % CFG.print_freq == 0 or step == (len(valid_loader)-1):\n",
    "            print('EVAL: [{0}/{1}] '\n",
    "                  'Elapsed {remain:s} '\n",
    "                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n",
    "                  .format(step, len(valid_loader),\n",
    "                          loss=losses,\n",
    "                          remain=timeSince(start, float(step+1)/len(valid_loader))))\n",
    "    predictions = np.concatenate(preds)\n",
    "    return losses.avg, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(folds, fold):\n",
    "\n",
    "    LOGGER.info(f\"========== fold: {fold} training ==========\")\n",
    "\n",
    "    # ====================================================\n",
    "    # loader\n",
    "    # ====================================================\n",
    "    train_folds = folds[folds['fold'] != fold].reset_index(drop=True)\n",
    "\n",
    "    valid_folds = folds[folds['fold'] == fold].reset_index(drop=True)\n",
    "    valid_labels = valid_folds[CFG.target_cols].values\n",
    "\n",
    "    train_dataset = TrainDataset(CFG, train_folds)\n",
    "    valid_dataset = TrainDataset(CFG, valid_folds)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset,\n",
    "                              batch_size=CFG.batch_size,\n",
    "                              shuffle=True,\n",
    "                              num_workers=CFG.num_workers, pin_memory=True, drop_last=True)\n",
    "    valid_loader = DataLoader(valid_dataset,\n",
    "                              batch_size=CFG.batch_size * 2,\n",
    "                              shuffle=False,\n",
    "                              num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\n",
    "\n",
    "    # ====================================================\n",
    "    # model & optimizer\n",
    "    # ====================================================\n",
    "    model = CustomModel(CFG, config_path=None, pretrained=True)\n",
    "    torch.save(model.config, os.path.join(OUTPUT_DIR, 'config.pth'))\n",
    "    model.to(device)\n",
    "\n",
    "    def get_optimizer_params(model, encoder_lr, decoder_lr, weight_decay=0.0):\n",
    "        param_optimizer = list(model.named_parameters())\n",
    "        no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "        optimizer_parameters = [\n",
    "            {'params': [p for n, p in model.model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "             'lr': encoder_lr, 'weight_decay': weight_decay},\n",
    "            {'params': [p for n, p in model.model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "             'lr': encoder_lr, 'weight_decay': 0.0},\n",
    "            {'params': [p for n, p in model.named_parameters() if \"model\" not in n],\n",
    "             'lr': decoder_lr, 'weight_decay': 0.0}\n",
    "        ]\n",
    "        return optimizer_parameters\n",
    "\n",
    "    optimizer_parameters = get_optimizer_params(model,\n",
    "                                                encoder_lr=CFG.encoder_lr,\n",
    "                                                decoder_lr=CFG.decoder_lr,\n",
    "                                                weight_decay=CFG.weight_decay)\n",
    "    optimizer = AdamW(optimizer_parameters, lr=CFG.encoder_lr,\n",
    "                      eps=CFG.eps, betas=CFG.betas)\n",
    "\n",
    "    # ====================================================\n",
    "    # scheduler\n",
    "    # ====================================================\n",
    "    def get_scheduler(cfg, optimizer, num_train_steps):\n",
    "        if cfg.scheduler == 'linear':\n",
    "            scheduler = get_linear_schedule_with_warmup(\n",
    "                optimizer, num_warmup_steps=cfg.num_warmup_steps, num_training_steps=num_train_steps\n",
    "            )\n",
    "        elif cfg.scheduler == 'cosine':\n",
    "            scheduler = get_cosine_schedule_with_warmup(\n",
    "                optimizer, num_warmup_steps=cfg.num_warmup_steps, num_training_steps=num_train_steps, num_cycles=cfg.num_cycles\n",
    "            )\n",
    "        return scheduler\n",
    "\n",
    "    num_train_steps = int(len(train_folds) / CFG.batch_size * CFG.epochs)\n",
    "    scheduler = get_scheduler(CFG, optimizer, num_train_steps)\n",
    "\n",
    "    # ====================================================\n",
    "    # loop\n",
    "    # ====================================================\n",
    "    criterion = nn.CrossEntropyLoss()  # МБ добавить веса в лосс\n",
    "\n",
    "    best_score = -1 * float('inf')\n",
    "\n",
    "    for epoch in range(CFG.epochs):\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        # train\n",
    "        avg_loss = train_fn(fold, train_loader, model,\n",
    "                            criterion, optimizer, epoch, scheduler, device)\n",
    "\n",
    "        # eval\n",
    "        avg_val_loss, predictions = valid_fn(\n",
    "            valid_loader, model, criterion, device)\n",
    "\n",
    "        # scoring\n",
    "        score = get_score(valid_labels, predictions)\n",
    "\n",
    "        elapsed = time.time() - start_time\n",
    "\n",
    "        LOGGER.info(\n",
    "            f'Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  avg_val_loss: {avg_val_loss:.4f}  time: {elapsed:.0f}s')\n",
    "        LOGGER.info(f'Epoch {epoch+1} - Score: {score:.4f}')\n",
    "\n",
    "        if best_score < score:\n",
    "            best_score = score\n",
    "            LOGGER.info(\n",
    "                f'Epoch {epoch+1} - Save Best Score: {best_score:.4f} Model')\n",
    "            torch.save({'model': model.state_dict(),\n",
    "                        'predictions': predictions},\n",
    "                       os.path.join(OUTPUT_DIR, f\"{CFG.model.replace('/', '-')}_fold{fold}_best.pth\"))\n",
    "\n",
    "    predictions = torch.load(os.path.join(OUTPUT_DIR, f\"{CFG.model.replace('/', '-')}_fold{fold}_best.pth\"),\n",
    "                             map_location=torch.device('cpu'))['predictions']\n",
    "\n",
    "    valid_folds[\"pred\"] = [np.argmax(el) for el in predictions]\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    return valid_folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "========== fold: 0 training ==========\n",
      "XLMRobertaConfig {\n",
      "  \"_name_or_path\": \"intfloat/multilingual-e5-large\",\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.39.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][0/122] Elapsed 0m 0s (remain 1m 57s) Loss: 3.0698(3.0698) Grad: inf  LR: 0.00002000  \n",
      "Epoch: [1][100/122] Elapsed 0m 42s (remain 0m 8s) Loss: 0.3916(0.7492) Grad: 1091246.2500  LR: 0.00001966  \n",
      "Epoch: [1][121/122] Elapsed 0m 50s (remain 0m 0s) Loss: 0.1607(0.6500) Grad: 363798.2812  LR: 0.00001951  \n",
      "EVAL: [0/16] Elapsed 0m 0s (remain 0m 7s) Loss: 0.8839(0.8839) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 - avg_train_loss: 0.6500  avg_val_loss: 0.2622  time: 57s\n",
      "Epoch 1 - Score: 0.9443\n",
      "Epoch 1 - Save Best Score: 0.9443 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [15/16] Elapsed 0m 6s (remain 0m 0s) Loss: 0.0010(0.2622) \n",
      "Epoch: [2][0/122] Elapsed 0m 0s (remain 1m 9s) Loss: 0.0003(0.0003) Grad: 2199.1697  LR: 0.00001950  \n",
      "Epoch: [2][100/122] Elapsed 0m 41s (remain 0m 8s) Loss: 0.0901(0.0946) Grad: 241652.7031  LR: 0.00001840  \n",
      "Epoch: [2][121/122] Elapsed 0m 50s (remain 0m 0s) Loss: 0.0005(0.1120) Grad: 313.2736  LR: 0.00001809  \n",
      "EVAL: [0/16] Elapsed 0m 0s (remain 0m 7s) Loss: 0.4737(0.4737) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 - avg_train_loss: 0.1120  avg_val_loss: 0.2931  time: 57s\n",
      "Epoch 2 - Score: 0.8483\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [15/16] Elapsed 0m 6s (remain 0m 0s) Loss: 0.0005(0.2931) \n",
      "Epoch: [3][0/122] Elapsed 0m 0s (remain 1m 8s) Loss: 0.0018(0.0018) Grad: 16904.5879  LR: 0.00001808  \n",
      "Epoch: [3][100/122] Elapsed 0m 41s (remain 0m 8s) Loss: 0.0001(0.1107) Grad: 39.5872  LR: 0.00001631  \n",
      "Epoch: [3][121/122] Elapsed 0m 50s (remain 0m 0s) Loss: 0.0000(0.0963) Grad: 6.9755  LR: 0.00001588  \n",
      "EVAL: [0/16] Elapsed 0m 0s (remain 0m 7s) Loss: 1.0773(1.0773) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 - avg_train_loss: 0.0963  avg_val_loss: 0.1742  time: 57s\n",
      "Epoch 3 - Score: 0.9921\n",
      "Epoch 3 - Save Best Score: 0.9921 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [15/16] Elapsed 0m 6s (remain 0m 0s) Loss: 0.0000(0.1742) \n",
      "Epoch: [4][0/122] Elapsed 0m 0s (remain 1m 7s) Loss: 0.0000(0.0000) Grad: 121.1076  LR: 0.00001586  \n",
      "Epoch: [4][100/122] Elapsed 0m 42s (remain 0m 8s) Loss: 0.0000(0.0078) Grad: 15.5362  LR: 0.00001360  \n",
      "Epoch: [4][121/122] Elapsed 0m 50s (remain 0m 0s) Loss: 0.0000(0.0065) Grad: 21.7788  LR: 0.00001309  \n",
      "EVAL: [0/16] Elapsed 0m 0s (remain 0m 7s) Loss: 1.1256(1.1256) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 - avg_train_loss: 0.0065  avg_val_loss: 0.1303  time: 58s\n",
      "Epoch 4 - Score: 0.9941\n",
      "Epoch 4 - Save Best Score: 0.9941 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [15/16] Elapsed 0m 6s (remain 0m 0s) Loss: 0.0000(0.1303) \n",
      "Epoch: [5][0/122] Elapsed 0m 0s (remain 1m 6s) Loss: 0.0000(0.0000) Grad: 21.3614  LR: 0.00001307  \n",
      "Epoch: [5][100/122] Elapsed 0m 42s (remain 0m 8s) Loss: 0.0000(0.0085) Grad: 9.5025  LR: 0.00001054  \n",
      "Epoch: [5][121/122] Elapsed 0m 50s (remain 0m 0s) Loss: 0.0000(0.0071) Grad: 9.0212  LR: 0.00001000  \n",
      "EVAL: [0/16] Elapsed 0m 0s (remain 0m 7s) Loss: 1.1256(1.1256) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 - avg_train_loss: 0.0071  avg_val_loss: 0.1297  time: 58s\n",
      "Epoch 5 - Score: 0.9941\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [15/16] Elapsed 0m 6s (remain 0m 0s) Loss: 0.0000(0.1297) \n",
      "Epoch: [6][0/122] Elapsed 0m 0s (remain 1m 1s) Loss: 0.0000(0.0000) Grad: 38.4241  LR: 0.00000997  \n",
      "Epoch: [6][100/122] Elapsed 0m 42s (remain 0m 8s) Loss: 0.0000(0.0090) Grad: 15.2017  LR: 0.00000743  \n",
      "Epoch: [6][121/122] Elapsed 0m 51s (remain 0m 0s) Loss: 0.0000(0.0075) Grad: 6.1796  LR: 0.00000691  \n",
      "EVAL: [0/16] Elapsed 0m 0s (remain 0m 7s) Loss: 1.1266(1.1266) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6 - avg_train_loss: 0.0075  avg_val_loss: 0.1295  time: 58s\n",
      "Epoch 6 - Score: 0.9941\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [15/16] Elapsed 0m 6s (remain 0m 0s) Loss: 0.0000(0.1295) \n",
      "Epoch: [7][0/122] Elapsed 0m 0s (remain 1m 2s) Loss: 0.0000(0.0000) Grad: 16.6559  LR: 0.00000689  \n",
      "Epoch: [7][100/122] Elapsed 0m 42s (remain 0m 8s) Loss: 0.0000(0.0000) Grad: 22.9074  LR: 0.00000457  \n",
      "Epoch: [7][121/122] Elapsed 0m 50s (remain 0m 0s) Loss: 0.0000(0.0079) Grad: 9.0501  LR: 0.00000412  \n",
      "EVAL: [0/16] Elapsed 0m 0s (remain 0m 7s) Loss: 1.1274(1.1274) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7 - avg_train_loss: 0.0079  avg_val_loss: 0.1293  time: 58s\n",
      "Epoch 7 - Score: 0.9941\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [15/16] Elapsed 0m 7s (remain 0m 0s) Loss: 0.0000(0.1293) \n",
      "Epoch: [8][0/122] Elapsed 0m 0s (remain 1m 9s) Loss: 0.0000(0.0000) Grad: 13.3966  LR: 0.00000410  \n",
      "Epoch: [8][100/122] Elapsed 0m 42s (remain 0m 8s) Loss: 0.0000(0.0096) Grad: 25.5464  LR: 0.00000224  \n",
      "Epoch: [8][121/122] Elapsed 0m 51s (remain 0m 0s) Loss: 0.0000(0.0080) Grad: 17.6206  LR: 0.00000191  \n",
      "EVAL: [0/16] Elapsed 0m 0s (remain 0m 7s) Loss: 1.1280(1.1280) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8 - avg_train_loss: 0.0080  avg_val_loss: 0.1292  time: 58s\n",
      "Epoch 8 - Score: 0.9941\n",
      "Epoch 8 - Save Best Score: 0.9941 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [15/16] Elapsed 0m 6s (remain 0m 0s) Loss: 0.0000(0.1292) \n",
      "Epoch: [9][0/122] Elapsed 0m 0s (remain 1m 11s) Loss: 0.0000(0.0000) Grad: 19.0996  LR: 0.00000189  \n",
      "Epoch: [9][100/122] Elapsed 0m 41s (remain 0m 8s) Loss: 0.0000(0.0096) Grad: 5.4324  LR: 0.00000067  \n",
      "Epoch: [9][121/122] Elapsed 0m 50s (remain 0m 0s) Loss: 0.0000(0.0080) Grad: 14.1617  LR: 0.00000049  \n",
      "EVAL: [0/16] Elapsed 0m 0s (remain 0m 7s) Loss: 1.1283(1.1283) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9 - avg_train_loss: 0.0080  avg_val_loss: 0.1292  time: 57s\n",
      "Epoch 9 - Score: 0.9941\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [15/16] Elapsed 0m 6s (remain 0m 0s) Loss: 0.0000(0.1292) \n",
      "Epoch: [10][0/122] Elapsed 0m 0s (remain 1m 0s) Loss: 0.0000(0.0000) Grad: 23.7634  LR: 0.00000048  \n",
      "Epoch: [10][100/122] Elapsed 0m 41s (remain 0m 8s) Loss: 0.0000(0.0097) Grad: 4.9210  LR: 0.00000001  \n",
      "Epoch: [10][121/122] Elapsed 0m 49s (remain 0m 0s) Loss: 0.0000(0.0080) Grad: 37.1088  LR: 0.00000000  \n",
      "EVAL: [0/16] Elapsed 0m 0s (remain 0m 8s) Loss: 1.1282(1.1282) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10 - avg_train_loss: 0.0080  avg_val_loss: 0.1292  time: 57s\n",
      "Epoch 10 - Score: 0.9941\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [15/16] Elapsed 0m 7s (remain 0m 0s) Loss: 0.0000(0.1292) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "========== fold: 0 result ==========\n",
      "Score: 0.9941\n",
      "========== fold: 1 training ==========\n",
      "XLMRobertaConfig {\n",
      "  \"_name_or_path\": \"intfloat/multilingual-e5-large\",\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.39.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][0/122] Elapsed 0m 0s (remain 1m 14s) Loss: 2.5630(2.5630) Grad: inf  LR: 0.00002000  \n",
      "Epoch: [1][100/122] Elapsed 0m 42s (remain 0m 8s) Loss: 0.0066(0.9074) Grad: 6338.2729  LR: 0.00001966  \n",
      "Epoch: [1][121/122] Elapsed 0m 51s (remain 0m 0s) Loss: 0.4828(0.7702) Grad: 862541.8125  LR: 0.00001951  \n",
      "EVAL: [0/16] Elapsed 0m 0s (remain 0m 8s) Loss: 0.0040(0.0040) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 - avg_train_loss: 0.7702  avg_val_loss: 0.0572  time: 59s\n",
      "Epoch 1 - Score: 0.9562\n",
      "Epoch 1 - Save Best Score: 0.9562 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [15/16] Elapsed 0m 6s (remain 0m 0s) Loss: 0.3555(0.0572) \n",
      "Epoch: [2][0/122] Elapsed 0m 0s (remain 1m 12s) Loss: 0.0038(0.0038) Grad: 16997.6992  LR: 0.00001950  \n",
      "Epoch: [2][100/122] Elapsed 0m 43s (remain 0m 8s) Loss: 0.0000(0.0436) Grad: 21.2400  LR: 0.00001840  \n",
      "Epoch: [2][121/122] Elapsed 0m 51s (remain 0m 0s) Loss: 0.0001(0.0635) Grad: 85.5990  LR: 0.00001809  \n",
      "EVAL: [0/16] Elapsed 0m 0s (remain 0m 8s) Loss: 0.0026(0.0026) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 - avg_train_loss: 0.0635  avg_val_loss: 0.1031  time: 59s\n",
      "Epoch 2 - Score: 0.9490\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [15/16] Elapsed 0m 7s (remain 0m 0s) Loss: 0.0134(0.1031) \n",
      "Epoch: [3][0/122] Elapsed 0m 0s (remain 1m 9s) Loss: 0.0001(0.0001) Grad: 706.9316  LR: 0.00001808  \n",
      "Epoch: [3][100/122] Elapsed 0m 42s (remain 0m 8s) Loss: 0.0000(0.0083) Grad: 20.1943  LR: 0.00001631  \n",
      "Epoch: [3][121/122] Elapsed 0m 51s (remain 0m 0s) Loss: 0.0000(0.0143) Grad: 18.3733  LR: 0.00001588  \n",
      "EVAL: [0/16] Elapsed 0m 0s (remain 0m 8s) Loss: 0.0001(0.0001) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 - avg_train_loss: 0.0143  avg_val_loss: 0.0492  time: 58s\n",
      "Epoch 3 - Score: 0.9857\n",
      "Epoch 3 - Save Best Score: 0.9857 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [15/16] Elapsed 0m 7s (remain 0m 0s) Loss: 0.0002(0.0492) \n",
      "Epoch: [4][0/122] Elapsed 0m 0s (remain 1m 12s) Loss: 0.0000(0.0000) Grad: 159.5550  LR: 0.00001586  \n",
      "Epoch: [4][100/122] Elapsed 0m 41s (remain 0m 8s) Loss: 0.0000(0.0076) Grad: 6.7682  LR: 0.00001361  \n",
      "Epoch: [4][121/122] Elapsed 0m 50s (remain 0m 0s) Loss: 0.0000(0.0102) Grad: 16.3533  LR: 0.00001310  \n",
      "EVAL: [0/16] Elapsed 0m 0s (remain 0m 8s) Loss: 0.0000(0.0000) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 - avg_train_loss: 0.0102  avg_val_loss: 0.0503  time: 57s\n",
      "Epoch 4 - Score: 0.9857\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [15/16] Elapsed 0m 7s (remain 0m 0s) Loss: 0.0001(0.0503) \n",
      "Epoch: [5][0/122] Elapsed 0m 0s (remain 1m 16s) Loss: 0.0000(0.0000) Grad: 29.0394  LR: 0.00001308  \n",
      "Epoch: [5][100/122] Elapsed 0m 41s (remain 0m 8s) Loss: 0.0000(0.0051) Grad: 20.1134  LR: 0.00001055  \n",
      "Epoch: [5][121/122] Elapsed 0m 50s (remain 0m 0s) Loss: 0.0000(0.0109) Grad: 5.3278  LR: 0.00001001  \n",
      "EVAL: [0/16] Elapsed 0m 0s (remain 0m 8s) Loss: 0.0001(0.0001) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 - avg_train_loss: 0.0109  avg_val_loss: 0.0512  time: 57s\n",
      "Epoch 5 - Score: 0.9857\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [15/16] Elapsed 0m 7s (remain 0m 0s) Loss: 0.0001(0.0512) \n",
      "Epoch: [6][0/122] Elapsed 0m 0s (remain 1m 9s) Loss: 0.0000(0.0000) Grad: 32.3456  LR: 0.00000999  \n",
      "Epoch: [6][100/122] Elapsed 0m 41s (remain 0m 8s) Loss: 0.0000(0.0052) Grad: 14.5139  LR: 0.00000744  \n",
      "Epoch: [6][121/122] Elapsed 0m 50s (remain 0m 0s) Loss: 0.0000(0.0113) Grad: 7.4578  LR: 0.00000692  \n",
      "EVAL: [0/16] Elapsed 0m 0s (remain 0m 8s) Loss: 0.0001(0.0001) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6 - avg_train_loss: 0.0113  avg_val_loss: 0.0519  time: 58s\n",
      "Epoch 6 - Score: 0.9857\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [15/16] Elapsed 0m 7s (remain 0m 0s) Loss: 0.0001(0.0519) \n",
      "Epoch: [7][0/122] Elapsed 0m 0s (remain 1m 15s) Loss: 0.0000(0.0000) Grad: 30.3901  LR: 0.00000690  \n",
      "Epoch: [7][100/122] Elapsed 0m 42s (remain 0m 8s) Loss: 0.0000(0.0138) Grad: 9.7173  LR: 0.00000458  \n",
      "Epoch: [7][121/122] Elapsed 0m 50s (remain 0m 0s) Loss: 0.0000(0.0114) Grad: 7.8206  LR: 0.00000414  \n",
      "EVAL: [0/16] Elapsed 0m 0s (remain 0m 9s) Loss: 0.0002(0.0002) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7 - avg_train_loss: 0.0114  avg_val_loss: 0.0523  time: 58s\n",
      "Epoch 7 - Score: 0.9857\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [15/16] Elapsed 0m 7s (remain 0m 0s) Loss: 0.0001(0.0523) \n",
      "Epoch: [8][0/122] Elapsed 0m 0s (remain 1m 15s) Loss: 0.0000(0.0000) Grad: 10.7804  LR: 0.00000412  \n",
      "Epoch: [8][100/122] Elapsed 0m 41s (remain 0m 8s) Loss: 0.0000(0.0140) Grad: 2.9370  LR: 0.00000225  \n",
      "Epoch: [8][121/122] Elapsed 0m 50s (remain 0m 0s) Loss: 0.0000(0.0116) Grad: 5.4613  LR: 0.00000192  \n",
      "EVAL: [0/16] Elapsed 0m 0s (remain 0m 8s) Loss: 0.0002(0.0002) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8 - avg_train_loss: 0.0116  avg_val_loss: 0.0524  time: 58s\n",
      "Epoch 8 - Score: 0.9857\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [15/16] Elapsed 0m 6s (remain 0m 0s) Loss: 0.0000(0.0524) \n",
      "Epoch: [9][0/122] Elapsed 0m 0s (remain 1m 15s) Loss: 0.0000(0.0000) Grad: 14.2263  LR: 0.00000191  \n",
      "Epoch: [9][100/122] Elapsed 0m 41s (remain 0m 8s) Loss: 0.0000(0.0141) Grad: 11.9517  LR: 0.00000068  \n",
      "Epoch: [9][121/122] Elapsed 0m 50s (remain 0m 0s) Loss: 0.0000(0.0117) Grad: 2.2643  LR: 0.00000050  \n",
      "EVAL: [0/16] Elapsed 0m 0s (remain 0m 8s) Loss: 0.0002(0.0002) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9 - avg_train_loss: 0.0117  avg_val_loss: 0.0525  time: 57s\n",
      "Epoch 9 - Score: 0.9857\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [15/16] Elapsed 0m 6s (remain 0m 0s) Loss: 0.0000(0.0525) \n",
      "Epoch: [10][0/122] Elapsed 0m 0s (remain 1m 21s) Loss: 0.0000(0.0000) Grad: 12.2797  LR: 0.00000049  \n",
      "Epoch: [10][100/122] Elapsed 0m 40s (remain 0m 8s) Loss: 0.0000(0.0141) Grad: 6.3166  LR: 0.00000002  \n",
      "Epoch: [10][121/122] Elapsed 0m 49s (remain 0m 0s) Loss: 0.0000(0.0117) Grad: 5.9471  LR: 0.00000000  \n",
      "EVAL: [0/16] Elapsed 0m 0s (remain 0m 8s) Loss: 0.0002(0.0002) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10 - avg_train_loss: 0.0117  avg_val_loss: 0.0525  time: 56s\n",
      "Epoch 10 - Score: 0.9857\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [15/16] Elapsed 0m 7s (remain 0m 0s) Loss: 0.0000(0.0525) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "========== fold: 1 result ==========\n",
      "Score: 0.9857\n",
      "========== fold: 2 training ==========\n",
      "XLMRobertaConfig {\n",
      "  \"_name_or_path\": \"intfloat/multilingual-e5-large\",\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.39.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][0/122] Elapsed 0m 0s (remain 1m 6s) Loss: 2.5087(2.5087) Grad: inf  LR: 0.00002000  \n",
      "Epoch: [1][100/122] Elapsed 0m 41s (remain 0m 8s) Loss: 1.0845(0.6780) Grad: 666560.8125  LR: 0.00001966  \n",
      "Epoch: [1][121/122] Elapsed 0m 50s (remain 0m 0s) Loss: 0.0304(0.6230) Grad: 11598.3047  LR: 0.00001951  \n",
      "EVAL: [0/16] Elapsed 0m 0s (remain 0m 8s) Loss: 0.0012(0.0012) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 - avg_train_loss: 0.6230  avg_val_loss: 0.0925  time: 58s\n",
      "Epoch 1 - Score: 0.9843\n",
      "Epoch 1 - Save Best Score: 0.9843 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [15/16] Elapsed 0m 7s (remain 0m 0s) Loss: 0.0033(0.0925) \n",
      "Epoch: [2][0/122] Elapsed 0m 0s (remain 1m 12s) Loss: 0.0545(0.0545) Grad: 588141.5000  LR: 0.00001950  \n",
      "Epoch: [2][100/122] Elapsed 0m 42s (remain 0m 8s) Loss: 0.0001(0.0528) Grad: 123.0307  LR: 0.00001840  \n",
      "Epoch: [2][121/122] Elapsed 0m 51s (remain 0m 0s) Loss: 0.0001(0.0522) Grad: 94.3123  LR: 0.00001809  \n",
      "EVAL: [0/16] Elapsed 0m 0s (remain 0m 8s) Loss: 0.0001(0.0001) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 - avg_train_loss: 0.0522  avg_val_loss: 0.0116  time: 59s\n",
      "Epoch 2 - Score: 0.9980\n",
      "Epoch 2 - Save Best Score: 0.9980 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [15/16] Elapsed 0m 7s (remain 0m 0s) Loss: 0.0001(0.0116) \n",
      "Epoch: [3][0/122] Elapsed 0m 0s (remain 1m 9s) Loss: 0.0001(0.0001) Grad: 243.8514  LR: 0.00001808  \n",
      "Epoch: [3][100/122] Elapsed 0m 42s (remain 0m 8s) Loss: 0.0024(0.0121) Grad: 20366.2539  LR: 0.00001631  \n",
      "Epoch: [3][121/122] Elapsed 0m 51s (remain 0m 0s) Loss: 0.0000(0.0100) Grad: 75.9015  LR: 0.00001588  \n",
      "EVAL: [0/16] Elapsed 0m 0s (remain 0m 8s) Loss: 0.0000(0.0000) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 - avg_train_loss: 0.0100  avg_val_loss: 0.0288  time: 59s\n",
      "Epoch 3 - Score: 0.9886\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [15/16] Elapsed 0m 7s (remain 0m 0s) Loss: 0.0000(0.0288) \n",
      "Epoch: [4][0/122] Elapsed 0m 0s (remain 1m 17s) Loss: 0.0000(0.0000) Grad: 37.3311  LR: 0.00001586  \n",
      "Epoch: [4][100/122] Elapsed 0m 42s (remain 0m 8s) Loss: 0.0000(0.0058) Grad: 22.2844  LR: 0.00001361  \n",
      "Epoch: [4][121/122] Elapsed 0m 51s (remain 0m 0s) Loss: 0.0000(0.0102) Grad: 21.7957  LR: 0.00001310  \n",
      "EVAL: [0/16] Elapsed 0m 0s (remain 0m 8s) Loss: 0.0000(0.0000) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 - avg_train_loss: 0.0102  avg_val_loss: 0.0235  time: 59s\n",
      "Epoch 4 - Score: 0.9886\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [15/16] Elapsed 0m 7s (remain 0m 0s) Loss: 0.0000(0.0235) \n",
      "Epoch: [5][0/122] Elapsed 0m 0s (remain 1m 9s) Loss: 0.0000(0.0000) Grad: 19.3857  LR: 0.00001308  \n",
      "Epoch: [5][100/122] Elapsed 0m 42s (remain 0m 8s) Loss: 0.0000(0.0114) Grad: 11.4406  LR: 0.00001055  \n",
      "Epoch: [5][121/122] Elapsed 0m 51s (remain 0m 0s) Loss: 0.0000(0.0095) Grad: 25.2484  LR: 0.00001001  \n",
      "EVAL: [0/16] Elapsed 0m 0s (remain 0m 8s) Loss: 0.0000(0.0000) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 - avg_train_loss: 0.0095  avg_val_loss: 0.0222  time: 59s\n",
      "Epoch 5 - Score: 0.9886\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [15/16] Elapsed 0m 7s (remain 0m 0s) Loss: 0.0000(0.0222) \n",
      "Epoch: [6][0/122] Elapsed 0m 0s (remain 1m 24s) Loss: 0.0000(0.0000) Grad: 44.4686  LR: 0.00000999  \n",
      "Epoch: [6][100/122] Elapsed 0m 43s (remain 0m 8s) Loss: 0.0000(0.0060) Grad: 13.4756  LR: 0.00000744  \n",
      "Epoch: [6][121/122] Elapsed 0m 52s (remain 0m 0s) Loss: 0.0000(0.0094) Grad: 8.4148  LR: 0.00000692  \n",
      "EVAL: [0/16] Elapsed 0m 0s (remain 0m 8s) Loss: 0.0000(0.0000) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6 - avg_train_loss: 0.0094  avg_val_loss: 0.0225  time: 59s\n",
      "Epoch 6 - Score: 0.9886\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [15/16] Elapsed 0m 7s (remain 0m 0s) Loss: 0.0000(0.0225) \n",
      "Epoch: [7][0/122] Elapsed 0m 0s (remain 1m 18s) Loss: 0.0000(0.0000) Grad: 131.6944  LR: 0.00000690  \n",
      "Epoch: [7][100/122] Elapsed 0m 42s (remain 0m 8s) Loss: 0.0000(0.0060) Grad: 12.7123  LR: 0.00000458  \n",
      "Epoch: [7][121/122] Elapsed 0m 51s (remain 0m 0s) Loss: 0.0000(0.0049) Grad: 17.8560  LR: 0.00000414  \n",
      "EVAL: [0/16] Elapsed 0m 0s (remain 0m 8s) Loss: 0.0000(0.0000) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7 - avg_train_loss: 0.0049  avg_val_loss: 0.0220  time: 59s\n",
      "Epoch 7 - Score: 0.9886\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [15/16] Elapsed 0m 6s (remain 0m 0s) Loss: 0.0000(0.0220) \n",
      "Epoch: [8][0/122] Elapsed 0m 0s (remain 1m 13s) Loss: 0.0000(0.0000) Grad: 29.8280  LR: 0.00000412  \n",
      "Epoch: [8][100/122] Elapsed 0m 42s (remain 0m 8s) Loss: 0.0000(0.0057) Grad: 10.8387  LR: 0.00000225  \n",
      "Epoch: [8][121/122] Elapsed 0m 51s (remain 0m 0s) Loss: 0.0000(0.0047) Grad: 7.6137  LR: 0.00000192  \n",
      "EVAL: [0/16] Elapsed 0m 0s (remain 0m 8s) Loss: 0.0000(0.0000) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8 - avg_train_loss: 0.0047  avg_val_loss: 0.0217  time: 59s\n",
      "Epoch 8 - Score: 0.9886\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [15/16] Elapsed 0m 7s (remain 0m 0s) Loss: 0.0000(0.0217) \n",
      "Epoch: [9][0/122] Elapsed 0m 0s (remain 1m 11s) Loss: 0.0000(0.0000) Grad: 12.2055  LR: 0.00000191  \n",
      "Epoch: [9][100/122] Elapsed 0m 42s (remain 0m 8s) Loss: 0.0000(0.0000) Grad: 24.8430  LR: 0.00000068  \n",
      "Epoch: [9][121/122] Elapsed 0m 51s (remain 0m 0s) Loss: 0.0000(0.0046) Grad: 15.8450  LR: 0.00000050  \n",
      "EVAL: [0/16] Elapsed 0m 0s (remain 0m 8s) Loss: 0.0000(0.0000) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9 - avg_train_loss: 0.0046  avg_val_loss: 0.0216  time: 59s\n",
      "Epoch 9 - Score: 0.9886\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [15/16] Elapsed 0m 7s (remain 0m 0s) Loss: 0.0000(0.0216) \n",
      "Epoch: [10][0/122] Elapsed 0m 0s (remain 1m 23s) Loss: 0.0000(0.0000) Grad: 15.3723  LR: 0.00000049  \n",
      "Epoch: [10][100/122] Elapsed 0m 43s (remain 0m 8s) Loss: 0.0000(0.0056) Grad: 9.1943  LR: 0.00000002  \n",
      "Epoch: [10][121/122] Elapsed 0m 51s (remain 0m 0s) Loss: 0.0000(0.0046) Grad: 11.2089  LR: 0.00000000  \n",
      "EVAL: [0/16] Elapsed 0m 0s (remain 0m 8s) Loss: 0.0000(0.0000) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10 - avg_train_loss: 0.0046  avg_val_loss: 0.0216  time: 59s\n",
      "Epoch 10 - Score: 0.9886\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [15/16] Elapsed 0m 7s (remain 0m 0s) Loss: 0.0000(0.0216) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "========== fold: 2 result ==========\n",
      "Score: 0.9980\n",
      "========== fold: 3 training ==========\n",
      "XLMRobertaConfig {\n",
      "  \"_name_or_path\": \"intfloat/multilingual-e5-large\",\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.39.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][0/122] Elapsed 0m 0s (remain 1m 8s) Loss: 2.9829(2.9829) Grad: inf  LR: 0.00002000  \n",
      "Epoch: [1][100/122] Elapsed 0m 42s (remain 0m 8s) Loss: 0.9364(0.6476) Grad: inf  LR: 0.00001966  \n",
      "Epoch: [1][121/122] Elapsed 0m 51s (remain 0m 0s) Loss: 0.0002(0.5483) Grad: 122.6515  LR: 0.00001951  \n",
      "EVAL: [0/16] Elapsed 0m 0s (remain 0m 8s) Loss: 0.0006(0.0006) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 - avg_train_loss: 0.5483  avg_val_loss: 0.0560  time: 59s\n",
      "Epoch 1 - Score: 0.9778\n",
      "Epoch 1 - Save Best Score: 0.9778 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [15/16] Elapsed 0m 7s (remain 0m 0s) Loss: 0.0027(0.0560) \n",
      "Epoch: [2][0/122] Elapsed 0m 0s (remain 1m 20s) Loss: 0.0002(0.0002) Grad: 654.5746  LR: 0.00001950  \n",
      "Epoch: [2][100/122] Elapsed 0m 42s (remain 0m 8s) Loss: 0.0000(0.0375) Grad: 121.8889  LR: 0.00001840  \n",
      "Epoch: [2][121/122] Elapsed 0m 51s (remain 0m 0s) Loss: 0.0000(0.0321) Grad: 13.5745  LR: 0.00001809  \n",
      "EVAL: [0/16] Elapsed 0m 0s (remain 0m 8s) Loss: 0.0000(0.0000) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 - avg_train_loss: 0.0321  avg_val_loss: 0.0610  time: 59s\n",
      "Epoch 2 - Score: 0.9666\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [15/16] Elapsed 0m 7s (remain 0m 0s) Loss: 0.0007(0.0610) \n",
      "Epoch: [3][0/122] Elapsed 0m 0s (remain 1m 8s) Loss: 0.4746(0.4746) Grad: inf  LR: 0.00001808  \n",
      "Epoch: [3][100/122] Elapsed 0m 42s (remain 0m 8s) Loss: 0.0000(0.0198) Grad: 22.4677  LR: 0.00001631  \n",
      "Epoch: [3][121/122] Elapsed 0m 51s (remain 0m 0s) Loss: 0.0001(0.0281) Grad: 291.5529  LR: 0.00001588  \n",
      "EVAL: [0/16] Elapsed 0m 0s (remain 0m 8s) Loss: 0.0001(0.0001) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 - avg_train_loss: 0.0281  avg_val_loss: 0.0003  time: 59s\n",
      "Epoch 3 - Score: 1.0000\n",
      "Epoch 3 - Save Best Score: 1.0000 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [15/16] Elapsed 0m 7s (remain 0m 0s) Loss: 0.0001(0.0003) \n",
      "Epoch: [4][0/122] Elapsed 0m 0s (remain 1m 18s) Loss: 0.0001(0.0001) Grad: 1012.1805  LR: 0.00001586  \n",
      "Epoch: [4][100/122] Elapsed 0m 42s (remain 0m 8s) Loss: 0.0000(0.0020) Grad: 41.3591  LR: 0.00001361  \n",
      "Epoch: [4][121/122] Elapsed 0m 51s (remain 0m 0s) Loss: 0.0000(0.0126) Grad: 12.4848  LR: 0.00001310  \n",
      "EVAL: [0/16] Elapsed 0m 0s (remain 0m 8s) Loss: 0.0000(0.0000) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 - avg_train_loss: 0.0126  avg_val_loss: 0.0002  time: 59s\n",
      "Epoch 4 - Score: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [15/16] Elapsed 0m 7s (remain 0m 0s) Loss: 0.0001(0.0002) \n",
      "Epoch: [5][0/122] Elapsed 0m 0s (remain 1m 11s) Loss: 0.0000(0.0000) Grad: 68.5035  LR: 0.00001308  \n",
      "Epoch: [5][100/122] Elapsed 0m 42s (remain 0m 8s) Loss: 0.0000(0.0129) Grad: 8.5847  LR: 0.00001055  \n",
      "Epoch: [5][121/122] Elapsed 0m 51s (remain 0m 0s) Loss: 0.0000(0.0106) Grad: 7.2427  LR: 0.00001001  \n",
      "EVAL: [0/16] Elapsed 0m 0s (remain 0m 8s) Loss: 0.0000(0.0000) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 - avg_train_loss: 0.0106  avg_val_loss: 0.0001  time: 59s\n",
      "Epoch 5 - Score: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [15/16] Elapsed 0m 7s (remain 0m 0s) Loss: 0.0000(0.0001) \n",
      "Epoch: [6][0/122] Elapsed 0m 0s (remain 1m 8s) Loss: 0.0000(0.0000) Grad: 28.5084  LR: 0.00000999  \n",
      "Epoch: [6][100/122] Elapsed 0m 42s (remain 0m 8s) Loss: 0.0000(0.0131) Grad: 7.6769  LR: 0.00000744  \n",
      "Epoch: [6][121/122] Elapsed 0m 51s (remain 0m 0s) Loss: 0.0000(0.0109) Grad: 23.3733  LR: 0.00000692  \n",
      "EVAL: [0/16] Elapsed 0m 0s (remain 0m 8s) Loss: 0.0000(0.0000) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6 - avg_train_loss: 0.0109  avg_val_loss: 0.0001  time: 59s\n",
      "Epoch 6 - Score: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [15/16] Elapsed 0m 6s (remain 0m 0s) Loss: 0.0000(0.0001) \n",
      "Epoch: [7][0/122] Elapsed 0m 0s (remain 1m 14s) Loss: 0.0000(0.0000) Grad: 13.3162  LR: 0.00000690  \n",
      "Epoch: [7][100/122] Elapsed 0m 42s (remain 0m 8s) Loss: 0.0000(0.0133) Grad: 6.4364  LR: 0.00000458  \n",
      "Epoch: [7][121/122] Elapsed 0m 51s (remain 0m 0s) Loss: 0.0000(0.0111) Grad: 6.1355  LR: 0.00000414  \n",
      "EVAL: [0/16] Elapsed 0m 0s (remain 0m 8s) Loss: 0.0000(0.0000) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7 - avg_train_loss: 0.0111  avg_val_loss: 0.0000  time: 58s\n",
      "Epoch 7 - Score: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [15/16] Elapsed 0m 7s (remain 0m 0s) Loss: 0.0000(0.0000) \n",
      "Epoch: [8][0/122] Elapsed 0m 0s (remain 1m 12s) Loss: 0.0000(0.0000) Grad: 29.4504  LR: 0.00000412  \n",
      "Epoch: [8][100/122] Elapsed 0m 42s (remain 0m 8s) Loss: 0.0000(0.0134) Grad: 11.5971  LR: 0.00000225  \n",
      "Epoch: [8][121/122] Elapsed 0m 51s (remain 0m 0s) Loss: 0.0000(0.0111) Grad: 8.7563  LR: 0.00000192  \n",
      "EVAL: [0/16] Elapsed 0m 0s (remain 0m 8s) Loss: 0.0000(0.0000) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8 - avg_train_loss: 0.0111  avg_val_loss: 0.0000  time: 59s\n",
      "Epoch 8 - Score: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [15/16] Elapsed 0m 7s (remain 0m 0s) Loss: 0.0000(0.0000) \n",
      "Epoch: [9][0/122] Elapsed 0m 0s (remain 1m 11s) Loss: 0.0000(0.0000) Grad: 22.9407  LR: 0.00000191  \n",
      "Epoch: [9][100/122] Elapsed 0m 42s (remain 0m 8s) Loss: 0.0000(0.0134) Grad: 5.9192  LR: 0.00000068  \n",
      "Epoch: [9][121/122] Elapsed 0m 51s (remain 0m 0s) Loss: 0.0000(0.0111) Grad: 5.3267  LR: 0.00000050  \n",
      "EVAL: [0/16] Elapsed 0m 0s (remain 0m 8s) Loss: 0.0000(0.0000) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9 - avg_train_loss: 0.0111  avg_val_loss: 0.0000  time: 59s\n",
      "Epoch 9 - Score: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [15/16] Elapsed 0m 6s (remain 0m 0s) Loss: 0.0000(0.0000) \n",
      "Epoch: [10][0/122] Elapsed 0m 0s (remain 1m 13s) Loss: 0.0000(0.0000) Grad: 17.7641  LR: 0.00000049  \n",
      "Epoch: [10][100/122] Elapsed 0m 43s (remain 0m 8s) Loss: 0.0000(0.0134) Grad: 11.4387  LR: 0.00000002  \n",
      "Epoch: [10][121/122] Elapsed 0m 51s (remain 0m 0s) Loss: 0.0000(0.0111) Grad: 14.0370  LR: 0.00000000  \n",
      "EVAL: [0/16] Elapsed 0m 0s (remain 0m 8s) Loss: 0.0000(0.0000) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10 - avg_train_loss: 0.0111  avg_val_loss: 0.0000  time: 59s\n",
      "Epoch 10 - Score: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [15/16] Elapsed 0m 6s (remain 0m 0s) Loss: 0.0000(0.0000) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "========== fold: 3 result ==========\n",
      "Score: 1.0000\n",
      "========== fold: 4 training ==========\n",
      "XLMRobertaConfig {\n",
      "  \"_name_or_path\": \"intfloat/multilingual-e5-large\",\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.39.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][0/122] Elapsed 0m 0s (remain 1m 14s) Loss: 2.1149(2.1149) Grad: inf  LR: 0.00002000  \n",
      "Epoch: [1][100/122] Elapsed 0m 43s (remain 0m 8s) Loss: 0.2037(0.6734) Grad: 226627.6406  LR: 0.00001966  \n",
      "Epoch: [1][121/122] Elapsed 0m 51s (remain 0m 0s) Loss: 0.0057(0.5990) Grad: 2971.7019  LR: 0.00001951  \n",
      "EVAL: [0/16] Elapsed 0m 0s (remain 0m 8s) Loss: 0.0245(0.0245) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 - avg_train_loss: 0.5990  avg_val_loss: 0.0778  time: 59s\n",
      "Epoch 1 - Score: 0.9822\n",
      "Epoch 1 - Save Best Score: 0.9822 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [15/16] Elapsed 0m 7s (remain 0m 0s) Loss: 0.0037(0.0778) \n",
      "Epoch: [2][0/122] Elapsed 0m 0s (remain 1m 10s) Loss: 0.0226(0.0226) Grad: 191911.7500  LR: 0.00001950  \n",
      "Epoch: [2][100/122] Elapsed 0m 42s (remain 0m 8s) Loss: 0.0002(0.0739) Grad: 99.0619  LR: 0.00001840  \n",
      "Epoch: [2][121/122] Elapsed 0m 51s (remain 0m 0s) Loss: 0.0005(0.0803) Grad: 216.0377  LR: 0.00001809  \n",
      "EVAL: [0/16] Elapsed 0m 0s (remain 0m 8s) Loss: 0.0002(0.0002) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 - avg_train_loss: 0.0803  avg_val_loss: 0.1134  time: 59s\n",
      "Epoch 2 - Score: 0.9837\n",
      "Epoch 2 - Save Best Score: 0.9837 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [15/16] Elapsed 0m 7s (remain 0m 0s) Loss: 0.0003(0.1134) \n",
      "Epoch: [3][0/122] Elapsed 0m 0s (remain 1m 8s) Loss: 0.0003(0.0003) Grad: 3941.6265  LR: 0.00001808  \n",
      "Epoch: [3][100/122] Elapsed 0m 42s (remain 0m 8s) Loss: 0.0000(0.0850) Grad: 52.5461  LR: 0.00001631  \n",
      "Epoch: [3][121/122] Elapsed 0m 50s (remain 0m 0s) Loss: 0.0000(0.0703) Grad: 6.1373  LR: 0.00001588  \n",
      "EVAL: [0/16] Elapsed 0m 0s (remain 0m 8s) Loss: 0.0000(0.0000) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 - avg_train_loss: 0.0703  avg_val_loss: 0.0085  time: 58s\n",
      "Epoch 3 - Score: 0.9901\n",
      "Epoch 3 - Save Best Score: 0.9901 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [15/16] Elapsed 0m 7s (remain 0m 0s) Loss: 0.0000(0.0085) \n",
      "Epoch: [4][0/122] Elapsed 0m 0s (remain 1m 9s) Loss: 0.0000(0.0000) Grad: 35.2826  LR: 0.00001586  \n",
      "Epoch: [4][100/122] Elapsed 0m 42s (remain 0m 8s) Loss: 0.0000(0.0394) Grad: 13.6164  LR: 0.00001361  \n",
      "Epoch: [4][121/122] Elapsed 0m 50s (remain 0m 0s) Loss: 0.0000(0.0333) Grad: 7.1376  LR: 0.00001310  \n",
      "EVAL: [0/16] Elapsed 0m 0s (remain 0m 8s) Loss: 0.0000(0.0000) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 - avg_train_loss: 0.0333  avg_val_loss: 0.0542  time: 58s\n",
      "Epoch 4 - Score: 0.9954\n",
      "Epoch 4 - Save Best Score: 0.9954 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [15/16] Elapsed 0m 7s (remain 0m 0s) Loss: 0.0000(0.0542) \n",
      "Epoch: [5][0/122] Elapsed 0m 0s (remain 1m 7s) Loss: 0.0000(0.0000) Grad: 110.6040  LR: 0.00001308  \n",
      "Epoch: [5][100/122] Elapsed 0m 42s (remain 0m 8s) Loss: 0.0000(0.0008) Grad: 11.7660  LR: 0.00001055  \n",
      "Epoch: [5][121/122] Elapsed 0m 51s (remain 0m 0s) Loss: 0.0000(0.0007) Grad: 7.6898  LR: 0.00001001  \n",
      "EVAL: [0/16] Elapsed 0m 0s (remain 0m 8s) Loss: 0.0000(0.0000) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 - avg_train_loss: 0.0007  avg_val_loss: 0.0503  time: 59s\n",
      "Epoch 5 - Score: 0.9954\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [15/16] Elapsed 0m 7s (remain 0m 0s) Loss: 0.0000(0.0503) \n",
      "Epoch: [6][0/122] Elapsed 0m 0s (remain 1m 16s) Loss: 0.0000(0.0000) Grad: 132.7482  LR: 0.00000999  \n",
      "Epoch: [6][100/122] Elapsed 0m 42s (remain 0m 8s) Loss: 0.0000(0.0020) Grad: 8.4197  LR: 0.00000744  \n",
      "Epoch: [6][121/122] Elapsed 0m 51s (remain 0m 0s) Loss: 0.0000(0.0017) Grad: 7.7845  LR: 0.00000692  \n",
      "EVAL: [0/16] Elapsed 0m 0s (remain 0m 8s) Loss: 0.0000(0.0000) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6 - avg_train_loss: 0.0017  avg_val_loss: 0.0515  time: 59s\n",
      "Epoch 6 - Score: 0.9954\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [15/16] Elapsed 0m 7s (remain 0m 0s) Loss: 0.0000(0.0515) \n",
      "Epoch: [7][0/122] Elapsed 0m 0s (remain 1m 41s) Loss: 0.0000(0.0000) Grad: 16.1647  LR: 0.00000690  \n",
      "Epoch: [7][100/122] Elapsed 0m 42s (remain 0m 8s) Loss: 0.0000(0.0024) Grad: 12.6033  LR: 0.00000458  \n",
      "Epoch: [7][121/122] Elapsed 0m 51s (remain 0m 0s) Loss: 0.0000(0.0020) Grad: 16.3733  LR: 0.00000414  \n",
      "EVAL: [0/16] Elapsed 0m 0s (remain 0m 8s) Loss: 0.0000(0.0000) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7 - avg_train_loss: 0.0020  avg_val_loss: 0.0519  time: 59s\n",
      "Epoch 7 - Score: 0.9954\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [15/16] Elapsed 0m 7s (remain 0m 0s) Loss: 0.0000(0.0519) \n",
      "Epoch: [8][0/122] Elapsed 0m 0s (remain 1m 12s) Loss: 0.0000(0.0000) Grad: 12.8488  LR: 0.00000412  \n",
      "Epoch: [8][100/122] Elapsed 0m 42s (remain 0m 8s) Loss: 0.2590(0.0026) Grad: nan  LR: 0.00000225  \n",
      "Epoch: [8][121/122] Elapsed 0m 51s (remain 0m 0s) Loss: 0.0000(0.0021) Grad: 5.8987  LR: 0.00000192  \n",
      "EVAL: [0/16] Elapsed 0m 0s (remain 0m 8s) Loss: 0.0000(0.0000) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8 - avg_train_loss: 0.0021  avg_val_loss: 0.0522  time: 59s\n",
      "Epoch 8 - Score: 0.9954\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [15/16] Elapsed 0m 7s (remain 0m 0s) Loss: 0.0000(0.0522) \n",
      "Epoch: [9][0/122] Elapsed 0m 0s (remain 1m 14s) Loss: 0.0000(0.0000) Grad: 30.7726  LR: 0.00000191  \n",
      "Epoch: [9][100/122] Elapsed 0m 42s (remain 0m 8s) Loss: 0.0000(0.0026) Grad: 5.0068  LR: 0.00000068  \n",
      "Epoch: [9][121/122] Elapsed 0m 51s (remain 0m 0s) Loss: 0.0000(0.0022) Grad: 7.7708  LR: 0.00000050  \n",
      "EVAL: [0/16] Elapsed 0m 0s (remain 0m 8s) Loss: 0.0000(0.0000) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9 - avg_train_loss: 0.0022  avg_val_loss: 0.0522  time: 59s\n",
      "Epoch 9 - Score: 0.9954\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [15/16] Elapsed 0m 7s (remain 0m 0s) Loss: 0.0000(0.0522) \n",
      "Epoch: [10][0/122] Elapsed 0m 1s (remain 2m 58s) Loss: 0.0000(0.0000) Grad: 21.3994  LR: 0.00000049  \n",
      "Epoch: [10][100/122] Elapsed 0m 43s (remain 0m 9s) Loss: 0.0000(0.0000) Grad: 18.6668  LR: 0.00000002  \n",
      "Epoch: [10][121/122] Elapsed 0m 52s (remain 0m 0s) Loss: 0.0000(0.0022) Grad: 15.6176  LR: 0.00000000  \n",
      "EVAL: [0/16] Elapsed 0m 0s (remain 0m 8s) Loss: 0.0000(0.0000) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10 - avg_train_loss: 0.0022  avg_val_loss: 0.0522  time: 59s\n",
      "Epoch 10 - Score: 0.9954\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [15/16] Elapsed 0m 7s (remain 0m 0s) Loss: 0.0000(0.0522) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "========== fold: 4 result ==========\n",
      "Score: 0.9954\n",
      "========== CV ==========\n",
      "Score: 0.9946\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    def get_result(oof_df):\n",
    "        labels = oof_df[CFG.target_cols].values\n",
    "        predictions = oof_df[\"pred\"].tolist()\n",
    "        score = f1_score(labels[:, 0], predictions, average=\"macro\")\n",
    "        LOGGER.info(f'Score: {score:.4f}')\n",
    "\n",
    "    if CFG.train:\n",
    "        oof_df = pd.DataFrame()\n",
    "        for fold in range(CFG.n_fold):\n",
    "            if fold in CFG.trn_fold:\n",
    "                _oof_df = train_loop(train, fold)\n",
    "                oof_df = pd.concat([oof_df, _oof_df])\n",
    "                LOGGER.info(f\"========== fold: {fold} result ==========\")\n",
    "                get_result(_oof_df)\n",
    "        oof_df = oof_df.reset_index(drop=True)\n",
    "        LOGGER.info(f\"========== CV ==========\")\n",
    "        get_result(oof_df)\n",
    "        oof_df.to_pickle(os.path.join(OUTPUT_DIR, 'oof_df.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
